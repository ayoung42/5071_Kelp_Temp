{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e23400-5d7d-417b-98f6-555eb9f5cbb0",
   "metadata": {},
   "source": [
    "# Before We Get Started, We Must Adress Acessing The Raw Data. \n",
    "\n",
    "This data is massive and cannot be uploaded to GitHub.\n",
    "\n",
    "### To Acess the Raw Data Download the Raw Data Folder Within This Google Drive. \n",
    "\"https://drive.google.com/drive/folders/1fnolrZtKUfsCs28yQUW587gDe7cv3SmO?usp=drive_link\"\n",
    "\n",
    "From there you will have to ensure you have specified the local path name to that \"raw_data\" folder when calling the dataimport function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac5899c-a03a-492b-88a8-0eb5f5b7c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec6c7f-f94f-420b-9905-2f97efd7e1dc",
   "metadata": {},
   "source": [
    "## EIM - WDOE Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecca28-d4cd-45c0-ab59-d329ed6fe409",
   "metadata": {},
   "source": [
    "### Load The Multiple Sites Data Sets Into Global Enviroments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcfea4d-aeaf-45ca-beec-76c9044e28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "eim_vars = [\n",
    "'GRG002_0',\n",
    "'GRG002_1',\n",
    "'BLL009',\n",
    "#'FID001',\n",
    "'SJF001_0', \n",
    "'SJF001_1',\n",
    "'SKG003', \n",
    "'ADM002', \n",
    "'ADM001_0', \n",
    "'ADM001_1',\n",
    "'SAR003_0',\n",
    "'SAR003_1', \n",
    "'PSS019', \n",
    "'ADM003_0',\n",
    "'ADM003_1', \n",
    "'PAH003',\n",
    "]\n",
    "\n",
    "path = \"/Users/carterwebb/Desktop/5700_DBM/DBM_Kelp/\" #grab your local file path and place here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200ebeb2-5b38-41d0-9518-aa9c7f905f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataimport(eim_vars, path):\n",
    "\n",
    "\n",
    "    for var in eim_vars:\n",
    "        globals()[var] = pd.read_csv(path + var + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e3b9b8d-f4be-45e8-8f29-15c50551bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataimport(eim_vars, path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbd84382-53d1-4d6e-9cdc-a060aef5433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8068 entries, 0 to 8067\n",
      "Data columns (total 30 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Study_ID                                        8068 non-null   object \n",
      " 1   Study_Name                                      8068 non-null   object \n",
      " 2   Location_ID                                     8068 non-null   object \n",
      " 3   Study_Specific_Location_ID                      8068 non-null   object \n",
      " 4   Location_Name                                   8068 non-null   object \n",
      " 5   Instrument_ID                                   8068 non-null   object \n",
      " 6   Field_Collection_Type                           8068 non-null   object \n",
      " 7   Field_Collector                                 8068 non-null   object \n",
      " 8   Time_Zone                                       8068 non-null   object \n",
      " 9   Field_Collection_Date                           8068 non-null   object \n",
      " 10  Field_Collection_Time                           8068 non-null   object \n",
      " 11  Field_Collection_Date_Time                      8068 non-null   object \n",
      " 12  Field_Collection_Comment                        0 non-null      float64\n",
      " 13  Matrix                                          8068 non-null   object \n",
      " 14  Source                                          8068 non-null   object \n",
      " 15  Depth_Value                                     8068 non-null   float64\n",
      " 16  Depth_Value_Units                               8068 non-null   object \n",
      " 17  Result_Parameter_Name                           8068 non-null   object \n",
      " 18  Result_Value                                    8068 non-null   float64\n",
      " 19  Result_Value_Units                              8068 non-null   object \n",
      " 20  Result_Data_Qualifier                           2908 non-null   object \n",
      " 21  Result_Method                                   8068 non-null   object \n",
      " 22  Result_Method_Description                       8068 non-null   object \n",
      " 23  Result_Comment                                  8068 non-null   object \n",
      " 24  Result_Data_Review_Status                       8068 non-null   object \n",
      " 25  Calculated_Latitude_Decimal_Degrees_NAD83HARN   8068 non-null   float64\n",
      " 26  Calculated_Longitude_Decimal_Degrees_NAD83HARN  8068 non-null   float64\n",
      " 27  Calculated_Land_Surface_Elevation_NAVD88_FT     0 non-null      float64\n",
      " 28  Record_Created_On                               8068 non-null   object \n",
      " 29  Continuous_Result_System_ID                     8068 non-null   int64  \n",
      "dtypes: float64(6), int64(1), object(23)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "PAH003.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25285931-2c81-4c22-9eda-8f31290d28b8",
   "metadata": {},
   "source": [
    "### Start Building Snake Case Data Sets\n",
    "This function builds standardized snake_case datasets by iterating over a dictionary of DataFrames and renaming columns according to the rename mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7abd1b65-989c-4d1a-bb60-09535da154fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eim_dfs(eim_vars, strict=True):\n",
    "\n",
    "    rename_map = {\n",
    "        \"Location_ID\": \"lid\",\n",
    "        \"Location_Name\": \"name\",\n",
    "        \"Instrument_ID\": \"int_id\",\n",
    "        \"Field_Collection_Date\": \"date\",\n",
    "        \"Field_Collection_Time\": \"time\",\n",
    "        \"Field_Collection_Date_Time\": \"date_time\",\n",
    "        \"Depth_Value\": \"depth\",\n",
    "        \"Depth_Value_Units\": \"dep_units\",\n",
    "        \"Result_Parameter_Name\": \"parameter\",\n",
    "        \"Result_Value\": \"result\",\n",
    "        \"Result_Value_Units\": \"result_units\",\n",
    "        \"Calculated_Latitude_Decimal_Degrees_NAD83HARN\": \"lat\",\n",
    "        \"Calculated_Longitude_Decimal_Degrees_NAD83HARN\": \"lon\",\n",
    "    }\n",
    "\n",
    "    clean = {}\n",
    "\n",
    "    for var in eim_vars:\n",
    "\n",
    "        df = globals().get(var)\n",
    "        if df is None:\n",
    "            raise ValueError(f\"{var} not found\")\n",
    "\n",
    "        # APPLIES BELOW #\n",
    "        # Flags missing columns, all the data sets should have the same syntax but \n",
    "        # As we found out through cleaning, that wasnt true. FID001 was a different syntax.\n",
    "        # This is the reason for introducing this block. \n",
    "        \n",
    "        if strict:\n",
    "            missing = [c for c in rename_map if c not in df.columns]\n",
    "            if missing:\n",
    "                raise KeyError(f\"{var} missing columns: {missing}\")\n",
    "\n",
    "        keep_cols = [c for c in rename_map if c in df.columns]\n",
    "\n",
    "        d = df[keep_cols].copy()\n",
    "        d = d.rename(columns=rename_map)\n",
    "\n",
    "        clean[var] = d\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9b3a1b6-3e0f-44c1-aba0-aa701a799a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = build_eim_dfs(eim_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e422e5-4504-4513-a28d-ac8fa04e4c62",
   "metadata": {},
   "source": [
    "### Lets Concat\n",
    "Strip keys, stack, then melt together, resetting index for sanity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1ae5cd-0c67-415a-ae0b-ad0a88b865ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_eim_clean(clean_dict, ignore_index=True):\n",
    "    return pd.concat(clean_dict.values(), axis=0, ignore_index=ignore_index, sort=False)\n",
    "    \n",
    "eim_all = concat_eim_clean(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f99f6e34-4d7a-47cd-972a-1b32d0557b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4330308 entries, 0 to 4330307\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   lid           object \n",
      " 1   name          object \n",
      " 2   int_id        object \n",
      " 3   date          object \n",
      " 4   time          object \n",
      " 5   date_time     object \n",
      " 6   depth         float64\n",
      " 7   dep_units     object \n",
      " 8   parameter     object \n",
      " 9   result        float64\n",
      " 10  result_units  object \n",
      " 11  lat           float64\n",
      " 12  lon           float64\n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 429.5+ MB\n"
     ]
    }
   ],
   "source": [
    "eim_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3881830-6094-4be4-bbde-2e06c2f7094a",
   "metadata": {},
   "source": [
    "### Stripping Unnecessary Information for Our Scope\n",
    " - First keep only temp\n",
    "\n",
    " - Then only 2014 - 2024 (same as kelp data)\n",
    "\n",
    " - Now only within the scope of kelp growth (~20m)\n",
    "\n",
    "You may ask, why not a function here like others, I had to keep checking step by step because of the messyness of this data at this point, didnt want to loose important data or introduce dirty data like in the Snake_Case function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c94cdc5c-be4d-4e85-a521-67de86bd82fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_eim = eim_all[\n",
    "    eim_all[\"parameter\"].str.contains(\"temperature\", case=False, na=False)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "015e1474-df37-4086-aad8-faed3509a3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Temperature, water'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_eim[\"parameter\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38188079-e197-4512-8cff-bba6ba4197d4",
   "metadata": {},
   "source": [
    "The data was unsorted so we had to sort first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e04f16f-7c31-4fb2-8dbb-df6386434e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/t5l3pg4j2j54njys9zrdzdxr0000gn/T/ipykernel_64431/344483683.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  temp_eim[\"date_time\"] = pd.to_datetime(temp_eim[\"date_time\"], errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "temp_eim[\"date_time\"] = pd.to_datetime(temp_eim[\"date_time\"], errors=\"coerce\")\n",
    "\n",
    "# Index was increadibly messy, had to look up .reset_index\n",
    "temp_eim = (\n",
    "    temp_eim\n",
    "    .sort_values([\"lid\", \"date_time\"])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bedd1f-c953-413f-a0e0-fa6055bf15b0",
   "metadata": {},
   "source": [
    "I still dont quite understand what these warnings mean, perhaps this is why date time was still object in anyalsis? This should serve as a remind to me to come back and check this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146da29e-66af-4d23-8f77-94d60016bb1e",
   "metadata": {},
   "source": [
    "Youll see I set it for 2014 - 2026, however, it seems only data between 2014 and 2024 are uploaded as of now, this is ok as that is same case for our kelp data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0df6493-e5b9-4e8f-b831-35444df43c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014 2024\n",
      "0.5 20.0\n"
     ]
    }
   ],
   "source": [
    "temp_eim_2014_2025 = temp_eim[\n",
    "    (temp_eim[\"date_time\"].dt.year.between(2014, 2026)) &\n",
    "    (temp_eim[\"depth\"] <= 20)\n",
    "].copy()\n",
    "\n",
    "print(\n",
    "    temp_eim_2014_2025[\"date_time\"].dt.year.min(),\n",
    "    temp_eim_2014_2025[\"date_time\"].dt.year.max()\n",
    ")\n",
    "\n",
    "print(\n",
    "    temp_eim_2014_2025[\"depth\"].min(),\n",
    "    temp_eim_2014_2025[\"depth\"].max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f89fa1d4-d0dc-42e6-880c-40c2a7c3dd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 32513 entries, 31013 to 427762\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   lid           32513 non-null  object        \n",
      " 1   name          32513 non-null  object        \n",
      " 2   int_id        32513 non-null  object        \n",
      " 3   date          32513 non-null  object        \n",
      " 4   time          32513 non-null  object        \n",
      " 5   date_time     32513 non-null  datetime64[ns]\n",
      " 6   depth         32513 non-null  float64       \n",
      " 7   dep_units     32513 non-null  object        \n",
      " 8   parameter     32513 non-null  object        \n",
      " 9   result        32513 non-null  float64       \n",
      " 10  result_units  32513 non-null  object        \n",
      " 11  lat           32513 non-null  float64       \n",
      " 12  lon           32513 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(4), object(8)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "temp_eim_2014_2025.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3b3e0-0ce3-49e2-af88-881e1c594ad2",
   "metadata": {},
   "source": [
    "## Clean temp_eim_2014_2025 df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0d5e8-0a42-49eb-81bf-5e88dce908b0",
   "metadata": {},
   "source": [
    "# Moving on to NWSC - MRC Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1864cf-ca76-4e16-b3b0-9ea946d081f1",
   "metadata": {},
   "source": [
    "## Load the Two Datasets we Need\n",
    " - Kelp Data\n",
    " - Lat Lon for Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ee7c0e0-688e-44c3-a978-207fdda39945",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vars= [\n",
    "   'AllBeds_Clean',\n",
    "    'latlon_sites'\n",
    "]\n",
    "\n",
    "path = \"/Users/carterwebb/Desktop/5700_DBM/DBM_Kelp/\" #grab your local file path and place here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36691f67-ebd7-4752-a9d4-6219e9fbd59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataimport(data_vars, path):\n",
    "\n",
    "\n",
    "    for var in data_vars:\n",
    "        globals()[var] = pd.read_csv(path + var + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4678ce74-b967-4873-bbb1-82751d380744",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataimport(data_vars, path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3cd45-466f-45f0-a99c-24a0f31c02a4",
   "metadata": {},
   "source": [
    "### Now Collapsing The Temp Data Into A Singular Value\n",
    "\n",
    "- Adapted from previous Kelp Work last quarter\n",
    "\n",
    "### Reason for? \n",
    "\n",
    "The current temperature column has many nans, however there are other temperature related columns (such as Temp Shore Edge) that have infomation in the same row. We want fill one singular value for temperature, based on the information we have.\n",
    "\n",
    "Throughout the survey's history, collection protocol has become more grainular.\n",
    "\n",
    "- 2016 - 2020: 1 Temperature Taken During Every Survey; df[\"Temp\"]\n",
    "- 2020 - 2022: 2 Temperature Taken During Every Survey; df[\"Ave Temp Shore Edge\", \"Ave Temp Water Edge\"]\n",
    "    - Important to know, these are not averages, they are recoded as such to reduce the size of the data files. \n",
    "- 2022 - 2024: 4 Temperature Taken During Every Survey; df[\"Temp1 Shore Edge\",\tTemp1 Water Edge\",\t\"Temp2 Shore Edge\",\t\"Temp2 Water Edge\"]\n",
    "\n",
    "We need to simplify all of these temperatures into singular values in order to do future analysis. \n",
    "\n",
    "### Logic/Algorithm Goal for First Portion of Funciton\n",
    "\n",
    " 1. If Temp exists → use Temp\n",
    "    \n",
    "    2 Else:\n",
    "       \n",
    "        a. If both Ave Temp Shore Edge and Ave Temp Water Edge exist:\n",
    "       \n",
    "              (Ave Temp Shore Edge + Ave Temp Water Edge) / 2\n",
    "       \n",
    "        b. Else:\n",
    "       \n",
    "              ((Temp1 Shore Edge + Temp2 Shore Edge)/2  +\n",
    "               (Temp1 Water Edge + Temp2 Water Edge)/2) / 2\n",
    "    \n",
    "\n",
    "This Was A Messy Hack Job, May Try To Clean Up Before The Final\n",
    "\n",
    "### Set Survey Date to Date Time\n",
    "### Lastly, Select for Only The NWSC Max Extent Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a140da8f-814d-464a-a9df-a87f71eb15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_temp_max_extent(df):\n",
    "\n",
    "    temp_cols = [\n",
    "        \"Temp\",\n",
    "        \"Ave Temp Shore Edge\",\n",
    "        \"Ave Temp Water Edge\",\n",
    "        \"Temp1 Shore Edge\",\n",
    "        \"Temp2 Shore Edge\",\n",
    "        \"Temp1 Water Edge\",\n",
    "        \"Temp2 Water Edge\",\n",
    "    ]\n",
    "\n",
    "    for c in temp_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "  \n",
    "    df[\"Temp_Final\"] = np.where(\n",
    "        ~df[\"Temp\"].isna(),\n",
    "        df[\"Temp\"],\n",
    "        np.where(\n",
    "            (~df[\"Ave Temp Shore Edge\"].isna()) &\n",
    "            (~df[\"Ave Temp Water Edge\"].isna()),\n",
    "            (df[\"Ave Temp Shore Edge\"] + df[\"Ave Temp Water Edge\"]) / 2,\n",
    "            (\n",
    "                ((df[\"Temp1 Shore Edge\"] + df[\"Temp2 Shore Edge\"]) / 2) +\n",
    "                ((df[\"Temp1 Water Edge\"] + df[\"Temp2 Water Edge\"]) / 2)\n",
    "            ) / 2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "    df[\"Survey Date\"] = pd.to_datetime(df[\"Survey Date\"])\n",
    "\n",
    "\n",
    "    \n",
    "    df_max = (\n",
    "        df[df[\"NWSC Max Ext\"] == 1]\n",
    "        .sort_values([\"Site Code\", \"Survey Year\", \"Acres\"],\n",
    "                     ascending=[True, True, False])\n",
    "        .drop_duplicates(subset=[\"Site Code\", \"Survey Year\"], keep=\"first\")\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    df_max[\"Temp_t\"] = df_max[\"Temp_Final\"]\n",
    "\n",
    "    return df_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40445723-6f5b-4516-8fe9-bd8ed22f8a8f",
   "metadata": {},
   "source": [
    "### Working Now On The Lat Lon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ebb9675-b4c5-418b-a0fa-991c16df3e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Aiston Preserve', 'Biz Point', 'Cherry Point-Gulf Rd',\n",
       "       'Clallam Bay', 'Coffin Rocks', \"Ebey's Landing\",\n",
       "       'Freshwater Bay 1', 'Freshwater Bay 2', 'Hat Island',\n",
       "       'Hoypus Point', 'Lowell', 'Lummi SW', 'North Beach East',\n",
       "       'Polnell Point', 'Possession Point', 'Shannon Point East',\n",
       "       'Shannon Point West'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AllBeds_Clean[\"Bed Name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fb11cb3-2160-4a01-83a0-df8a187bb010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Clallam Bay', 'FWB2/Observatory Point', 'FWB3/Extension',\n",
       "       'North Beach', 'Edmonds Dive Park', 'Edmonds North', 'Meadowdale',\n",
       "       'Mukilteo', 'Hat Island', 'Hat Island West', 'Possession Point',\n",
       "       \"Ebey's\", 'Polnell', 'Lowell Point Camano State Park', 'Hoypus',\n",
       "       'Coffin Rocks', 'Biz Point', 'Shannon Point West',\n",
       "       'Shannon Point East', 'Lummi SW', 'Aiston', 'Cherry Point/Gulf Rd',\n",
       "       'Point Whitehorn', 'FWB1', 'Biz Point South', 'Lone Tree Point'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latlon_sites[\"Location\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619c374-9619-4de9-bd68-477c62ec0063",
   "metadata": {},
   "source": [
    "For now we dont really mind if NaN values pop up for unshared bed locations, so a basic .map() will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a83ff104-2539-41a4-9ff2-2be34f055f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    'Aiston': 'Aiston Preserve', \n",
    "    'Biz Point': 'Biz Point', \n",
    "    'Cherry Point/Gulf Rd': 'Cherry Point-Gulf Rd',\n",
    "    'Clallam Bay': 'Clallam Bay', \n",
    "    'Coffin Rocks': 'Coffin Rocks', \n",
    "    \"Ebey's\": \"Ebey's Landing\",\n",
    "    'FWB1': 'Freshwater Bay 1', \n",
    "    'FWB2/Observatory Point' : 'Freshwater Bay 2', \n",
    "    'Hat Island': 'Hat Island',\n",
    "    'Hoypus': 'Hoypus Point', \n",
    "    'Lowell Point Camano State Park': 'Lowell', \n",
    "    'Lummi SW': 'Lummi SW',\n",
    "    'North Beach': 'North Beach East',\n",
    "    'Polnell': 'Polnell Point', \n",
    "    'Possession Point': 'Possession Point', \n",
    "    'Shannon Point East': 'Shannon Point East',\n",
    "    'Shannon Point West': 'Shannon Point West'}\n",
    "\n",
    "latlon_sites['Location'] = latlon_sites['Location'].map(rename_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d807e9c-2244-4762-a975-e46d18f86c87",
   "metadata": {},
   "source": [
    "Drop NaNs and Left Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c69cfd0e-f4aa-4781-8590-98af1aa66e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_clean = (\n",
    "    latlon_sites[[\"Location\", \"lat\", \"lon\"]]\n",
    "    .dropna(subset=[\"Location\"])\n",
    "    .drop_duplicates(\"Location\")\n",
    ")\n",
    "\n",
    "AllBeds_latlon = AllBeds_Clean.merge(\n",
    "    latlon_clean,\n",
    "    left_on=\"Bed Name\",\n",
    "    right_on=\"Location\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3278b33-b52d-44cf-9dd9-8cff86ff7239",
   "metadata": {},
   "source": [
    "Apply The Previous Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51feaa5b-30c3-4f25-b8e5-6366a2b8077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllBeds_latlon = prepare_temp_max_extent(AllBeds_latlon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9b06d-2771-43cc-a567-1af3f7e14d9a",
   "metadata": {},
   "source": [
    "## Finally Building Our Large Concatenated Fact Table With EIM - WDOE & NWSC - MRC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46727b8a-7b55-4cb5-8127-f6b85930d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fact_table_clean(AllBeds_latlon, temp_eim_2014_2025):\n",
    "\n",
    "    ### Prep EIM ###\n",
    "    \n",
    "    eim = temp_eim_2014_2025.copy()\n",
    "\n",
    "    # datetime\n",
    "    eim[\"date_time\"] = pd.to_datetime(eim[\"date_time\"], errors=\"coerce\")\n",
    "\n",
    "    # create nwsc_max in order to partition the fact sheet easier\n",
    "    eim[\"NWSC_Max\"] = 0\n",
    "    \n",
    "    # Drop Columns on eim\n",
    "    eim = eim[[\n",
    "        \"lid\",\n",
    "        \"name\", \n",
    "        \"date_time\",\n",
    "        \"parameter\", \n",
    "        \"result\", \"result_units\",\n",
    "        \"depth\", \"dep_units\",\n",
    "        \"lat\", \"lon\", \n",
    "        \"NWSC_Max\"\n",
    "    ]].copy()\n",
    "\n",
    "\n",
    "    ### Prep AllBeds ###\n",
    "\n",
    "    beds = AllBeds_latlon.copy()\n",
    "\n",
    "    # datetime\n",
    "    beds[\"Survey Date\"] = pd.to_datetime(beds[\"Survey Date\"], errors=\"coerce\")\n",
    "\n",
    "    # drop columns and rename to match eim\n",
    "    beds_clean = pd.DataFrame({\n",
    "        \"lid\": beds[\"Site Code\"],  # Site Code -> lid\n",
    "        \"name\": beds[\"Bed Name\"],     # Bed Name -> name\n",
    "        \"date_time\": beds[\"Survey Date\"],  # Survey Date -> date_time\n",
    "        \"parameter\": \"temperature\",     # constant\n",
    "        \"result\": beds[\"Temp_Final\"],   # Temp_Final -> result\n",
    "        \"result_units\": \"celsius\",     # constant\n",
    "        \"depth\": 0,   # constant\n",
    "        \"dep_units\": \"m\",  # constant\n",
    "        \"lat\": beds[\"lat\"],\n",
    "        \"lon\": beds[\"lon\"],\n",
    "        \"NWSC_Max\": beds[\"NWSC Max Ext\"]\n",
    "    })\n",
    "\n",
    "\n",
    "    ### Concat ###\n",
    "\n",
    "    fact_table_clean = pd.concat([beds_clean, eim], ignore_index=True)\n",
    "\n",
    "    return fact_table_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cec0ebd-c1b2-4a1e-b05b-aad0d05ab680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32650 entries, 0 to 32649\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   lid           32650 non-null  object        \n",
      " 1   name          32650 non-null  object        \n",
      " 2   date_time     32650 non-null  datetime64[ns]\n",
      " 3   parameter     32650 non-null  object        \n",
      " 4   result        32632 non-null  float64       \n",
      " 5   result_units  32650 non-null  object        \n",
      " 6   depth         32650 non-null  float64       \n",
      " 7   dep_units     32650 non-null  object        \n",
      " 8   lat           32650 non-null  float64       \n",
      " 9   lon           32650 non-null  float64       \n",
      " 10  NWSC_Max      32650 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(5), object(5)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "fact_table_clean = build_fact_table_clean(AllBeds_latlon, temp_eim_2014_2025)\n",
    "fact_table_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15769005-d063-44b3-a98d-03ddf06541ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lid</th>\n",
       "      <th>name</th>\n",
       "      <th>date_time</th>\n",
       "      <th>parameter</th>\n",
       "      <th>result</th>\n",
       "      <th>result_units</th>\n",
       "      <th>depth</th>\n",
       "      <th>dep_units</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>NWSC_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AISP</td>\n",
       "      <td>Aiston Preserve</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>temperature</td>\n",
       "      <td>16.000</td>\n",
       "      <td>celsius</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m</td>\n",
       "      <td>48.680355</td>\n",
       "      <td>-122.629921</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AISP</td>\n",
       "      <td>Aiston Preserve</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>temperature</td>\n",
       "      <td>16.000</td>\n",
       "      <td>celsius</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m</td>\n",
       "      <td>48.680355</td>\n",
       "      <td>-122.629921</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AISP</td>\n",
       "      <td>Aiston Preserve</td>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>temperature</td>\n",
       "      <td>15.150</td>\n",
       "      <td>celsius</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m</td>\n",
       "      <td>48.680355</td>\n",
       "      <td>-122.629921</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AISP</td>\n",
       "      <td>Aiston Preserve</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>temperature</td>\n",
       "      <td>14.440</td>\n",
       "      <td>celsius</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m</td>\n",
       "      <td>48.680355</td>\n",
       "      <td>-122.629921</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AISP</td>\n",
       "      <td>Aiston Preserve</td>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>temperature</td>\n",
       "      <td>20.000</td>\n",
       "      <td>celsius</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m</td>\n",
       "      <td>48.680355</td>\n",
       "      <td>-122.629921</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>SHPT-W</td>\n",
       "      <td>Shannon Point West</td>\n",
       "      <td>2020-09-18</td>\n",
       "      <td>temperature</td>\n",
       "      <td>11.400</td>\n",
       "      <td>celsius</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m</td>\n",
       "      <td>48.507718</td>\n",
       "      <td>-122.688303</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>SHPT-W</td>\n",
       "      <td>Shannon Point West</td>\n",
       "      <td>2021-09-19</td>\n",
       "      <td>temperature</td>\n",
       "      <td>10.695</td>\n",
       "      <td>celsius</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m</td>\n",
       "      <td>48.507718</td>\n",
       "      <td>-122.688303</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>SHPT-W</td>\n",
       "      <td>Shannon Point West</td>\n",
       "      <td>2022-08-12</td>\n",
       "      <td>temperature</td>\n",
       "      <td>11.945</td>\n",
       "      <td>celsius</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m</td>\n",
       "      <td>48.507718</td>\n",
       "      <td>-122.688303</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>SHPT-W</td>\n",
       "      <td>Shannon Point West</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>temperature</td>\n",
       "      <td>13.150</td>\n",
       "      <td>celsius</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m</td>\n",
       "      <td>48.507718</td>\n",
       "      <td>-122.688303</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>SHPT-W</td>\n",
       "      <td>Shannon Point West</td>\n",
       "      <td>2024-08-16</td>\n",
       "      <td>temperature</td>\n",
       "      <td>12.200</td>\n",
       "      <td>celsius</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m</td>\n",
       "      <td>48.507718</td>\n",
       "      <td>-122.688303</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lid                name  date_time    parameter  result result_units  \\\n",
       "0      AISP     Aiston Preserve 2018-07-31  temperature  16.000      celsius   \n",
       "1      AISP     Aiston Preserve 2019-08-01  temperature  16.000      celsius   \n",
       "2      AISP     Aiston Preserve 2020-07-20  temperature  15.150      celsius   \n",
       "3      AISP     Aiston Preserve 2021-07-27  temperature  14.440      celsius   \n",
       "4      AISP     Aiston Preserve 2022-07-31  temperature  20.000      celsius   \n",
       "..      ...                 ...        ...          ...     ...          ...   \n",
       "132  SHPT-W  Shannon Point West 2020-09-18  temperature  11.400      celsius   \n",
       "133  SHPT-W  Shannon Point West 2021-09-19  temperature  10.695      celsius   \n",
       "134  SHPT-W  Shannon Point West 2022-08-12  temperature  11.945      celsius   \n",
       "135  SHPT-W  Shannon Point West 2023-09-01  temperature  13.150      celsius   \n",
       "136  SHPT-W  Shannon Point West 2024-08-16  temperature  12.200      celsius   \n",
       "\n",
       "     depth dep_units        lat         lon  NWSC_Max  \n",
       "0      0.0         m  48.680355 -122.629921       1.0  \n",
       "1      0.0         m  48.680355 -122.629921       1.0  \n",
       "2      0.0         m  48.680355 -122.629921       1.0  \n",
       "3      0.0         m  48.680355 -122.629921       1.0  \n",
       "4      0.0         m  48.680355 -122.629921       1.0  \n",
       "..     ...       ...        ...         ...       ...  \n",
       "132    0.0         m  48.507718 -122.688303       1.0  \n",
       "133    0.0         m  48.507718 -122.688303       1.0  \n",
       "134    0.0         m  48.507718 -122.688303       1.0  \n",
       "135    0.0         m  48.507718 -122.688303       1.0  \n",
       "136    0.0         m  48.507718 -122.688303       1.0  \n",
       "\n",
       "[137 rows x 11 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwsc = fact_table_clean[fact_table_clean[\"NWSC_Max\"] == 1]\n",
    "nwsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a10524-097e-4450-ba72-18e0daeaeb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_table_clean.to_csv(\"/Users/carterwebb/Desktop/5700_DBM/DBM_Kelp/fact_table_clean.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
